{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/e2/f602e3f584503f03e0389491b251464f8ecfe2596ac86e6b9068fe7419d3/twitter-1.18.0-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: twitter\n",
      "Successfully installed twitter-1.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Collecting requests-oauthlib>=0.7.0 (from tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.2.0 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/e9/57/8a9889d49d0d77905af5a7524fb2b468d2ef5fc723684f51f5ca63efed0d/scikit_learn-0.21.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/colinechabloz/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\n",
      "Successfully installed scikit-learn-0.21.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy, json\n",
    "import twitter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import time\n",
    "import ssl\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import Timeout, ConnectionError\n",
    "from requests.packages.urllib3.exceptions import ReadTimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1\n",
    "Source: https://github.com/feconroses/gather-tweets-from-stream/blob/master/stream.py\n",
    "\n",
    "This Python script allows you to connect to the Twitter Standard Search API, gather historical tweets from up to 7 days ago that contain a specific keyword, hashtag or mention, and save them into a CSV file under the following column categories:\n",
    "\n",
    "- Tweet content: text of the tweet\n",
    "- Date: date and hour of the tweet\n",
    "- User: name of the author of the tweet\n",
    "- Source used to post the tweet (for example, Twitter Web Client or Buffer)\n",
    "- Tweet ID\n",
    "- Tweet URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"3395163311-L3kfrmBLjeT2wid9IUIrmhWhg9xhwoWIuGVdtdB\"\n",
    "access_secret = \"c7yzOy6ZP7fSspILaPkPFhaIp65P9g2rzcSTqG4OgSqDD\"\n",
    "consumer_key = \"3h2dbtJm7BchEn8NhszxhqGXF\"\n",
    "consumer_secret = \"kLLwKYTIZg4FyNlECC3jxI34ITTTMI22KWuq3uZjpzZid2kLVR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling authentication with Twitter\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper for the API provided by Twitter\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test video Siraj Raval\n",
    "public_tweets = api.search('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'textblob' from 'textblob' (/Users/colinechabloz/anaconda3/lib/python3.7/site-packages/textblob/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-90e3ab4cb518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#test video Siraj Raval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpublic_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'textblob' from 'textblob' (/Users/colinechabloz/anaconda3/lib/python3.7/site-packages/textblob/__init__.py)"
     ]
    }
   ],
   "source": [
    "#test video Siraj Raval\n",
    "from textblob import textblob\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n",
    "    analysis = textblob(tweet.text)\n",
    "    print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the keywords, hashtag or mentions we want to listen\n",
    "#keywords = [\"#SEO\", \"SEO\"]\n",
    "keywords = [\"Trump\", \"Ukraine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name for CSV file  where the tweets will be saved\n",
    "filename = \"tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to implement StreamListener to use Tweepy to listen to Twitter\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            #saves the tweet object\n",
    "            tweet_object = status\n",
    "            \n",
    "            # checks if its a extended tweet (>140 characters)\n",
    "            if 'extended_tweet' in tweet_object._json:\n",
    "                tweet = tweet_object.extended_tweet['full_text']\n",
    "            else: \n",
    "                tweet = tweet_object.text\n",
    "                \n",
    "            '''Convert all named and numeric character references\n",
    "            (e.g. &gt;, &#62;, &#x3e;) in the string s to the\n",
    "            corresponding Unicode characters'''\n",
    "            tweet = (tweet.replace('&amp;', '&').replace('&lt;', '<')\n",
    "                     .replace('&gt;', '>').replace('&quot;', '\"')\n",
    "                     .replace('&#39;', \"'\").replace(';', \" \")\n",
    "                     .replace(r'\\u', \" \"))\n",
    "            \n",
    "            #Save the keyword that matches the stream\n",
    "            keyword_matches = []\n",
    "            for word in keywords: \n",
    "                if word.lower() in tweet.lower():\n",
    "                    keyword_matches.extend([word])\n",
    "            \n",
    "            keywords_strings = \", \".join(str(x) for x in keyword_matches)\n",
    "            \n",
    "            #Save other information from the tweet\n",
    "            user = status.author.screen_name\n",
    "            timeTweet = status.created_at\n",
    "            source = status.source\n",
    "            tweetId = status.id\n",
    "            tweetUrl = \"https://twitter.com/statuses/\" + str(tweetId)\n",
    "            \n",
    "            #Exclude retweets, too many mentions and too many hashtags\n",
    "            if not any((('RT @' in tweet, 'RT' in tweet,\n",
    "                       tweet.count('@') >= 2, tweet.count('#') >= 3))):\n",
    "                \n",
    "                #Saves the tweet information in a new row of the CSV file \n",
    "                writer.writerow([tweet, keywords_strings, timeTweet,\n",
    "                                user, source, tweetId, tweetUrl])\n",
    "        \n",
    "        except Exception as e: \n",
    "            print('Encountered Exception:', e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work():\n",
    "    #Opening a CSV file to save the gathered tweets\n",
    "    with open(filename+\".csv\", 'w') as file:\n",
    "        global writer\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        #Add a header row to the CSV\n",
    "        writer.writerow([\"Tweet\", \"Matched Keywords\", \"Date\", \"User\", \"Source\", \"Tweet ID\", \"Tweet URL\"])\n",
    "        \n",
    "        #Initializing the twitter streap Stream\n",
    "        try:\n",
    "            streamingAPI = tweepy.streaming.Stream(auth, StreamListener())\n",
    "            streamingAPI.filter(track=keywords)\n",
    "        \n",
    "        # Stop temporarily when hitting Twitter rate Limit\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"RateLimitError...waiting ~15 minutes to continue\")\n",
    "            time.sleep(1001)\n",
    "            streamingAPI = tweepy.streaming.Stream(auth, StreamListener())\n",
    "            streamingAPI.filter(track=[keywords])\n",
    "        \n",
    "        # Stop temporarily when getting a timeout or connection error\n",
    "        except (Timeout, ssl.SSLError, ReadTimeoutError,\n",
    "                ConnectionError) as exc:\n",
    "            print(\"Timeout/connection error...waiting ~15 minutes to continue\")\n",
    "            time.sleep(1001)\n",
    "            streamingAPI = tweepy.streaming.Stream(auth, StreamListener())\n",
    "            streamingAPI.filter(track=[keywords])\n",
    "        \n",
    "        # Stop temporarily when getting other errors\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Failed to send request:' in e.reason:\n",
    "                print(\"Time out error caught.\")\n",
    "                time.sleep(1001)\n",
    "                streamingAPI = tweepy.streaming.Stream(auth, StreamListener())\n",
    "                streamingAPI.filter(track=[keywords])\n",
    "            else:\n",
    "                print(\"Other error with this user...passing\")\n",
    "                pass\n",
    "    if __name__ =='__main__':\n",
    "        \n",
    "        work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tweets.csv can be found in same folder as Jupyter Notebook is located. View content, length?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetscsv = pd.read_csv('/Users/colinechabloz/Desktop/Springboard/CapstoneProject2/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Source</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Evolution of Google‚Äôs News Ranking Algorithm #...</td>\n",
       "      <td>2019-10-31 23:58:25</td>\n",
       "      <td>MikeBlazerX</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>1190055454614982659</td>\n",
       "      <td>https://twitter.com/statuses/1190055454614982659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How To Get More Traffic! https://t.co/cd2oybk2...</td>\n",
       "      <td>2019-10-31 23:52:08</td>\n",
       "      <td>myostaff</td>\n",
       "      <td>Paper.li</td>\n",
       "      <td>1190053873471381504</td>\n",
       "      <td>https://twitter.com/statuses/1190053873471381504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üéÉüéÉTrick or Treat! Just like Frankenstein's mon...</td>\n",
       "      <td>2019-10-31 23:51:09</td>\n",
       "      <td>webitmd</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>1190053628297535490</td>\n",
       "      <td>https://twitter.com/statuses/1190053628297535490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Releases Study of 2019 Holiday Shopping...</td>\n",
       "      <td>2019-10-31 23:51:03</td>\n",
       "      <td>joeybalestrino</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>1190053601714089984</td>\n",
       "      <td>https://twitter.com/statuses/1190053601714089984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#seo &amp; #funny Prison Economy Spirals As Price ...</td>\n",
       "      <td>2019-10-31 23:50:41</td>\n",
       "      <td>Seo_Santos</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>1190053510957678592</td>\n",
       "      <td>https://twitter.com/statuses/1190053510957678592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Tweet Content                 Date  \\\n",
       "0  Evolution of Google‚Äôs News Ranking Algorithm #...  2019-10-31 23:58:25   \n",
       "1  How To Get More Traffic! https://t.co/cd2oybk2...  2019-10-31 23:52:08   \n",
       "2  üéÉüéÉTrick or Treat! Just like Frankenstein's mon...  2019-10-31 23:51:09   \n",
       "3  Google Releases Study of 2019 Holiday Shopping...  2019-10-31 23:51:03   \n",
       "4  #seo & #funny Prison Economy Spirals As Price ...  2019-10-31 23:50:41   \n",
       "\n",
       "             User    Source             Tweet ID  \\\n",
       "0     MikeBlazerX     IFTTT  1190055454614982659   \n",
       "1        myostaff  Paper.li  1190053873471381504   \n",
       "2         webitmd   HubSpot  1190053628297535490   \n",
       "3  joeybalestrino     IFTTT  1190053601714089984   \n",
       "4      Seo_Santos     IFTTT  1190053510957678592   \n",
       "\n",
       "                                          Tweet URL  \n",
       "0  https://twitter.com/statuses/1190055454614982659  \n",
       "1  https://twitter.com/statuses/1190053873471381504  \n",
       "2  https://twitter.com/statuses/1190053628297535490  \n",
       "3  https://twitter.com/statuses/1190053601714089984  \n",
       "4  https://twitter.com/statuses/1190053510957678592  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetscsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Doesn't seem to contain \"Trump\" and \"Ukraine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2\n",
    "\n",
    "Source: https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Preparing The Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A.2: Authenticating our Python script**\n",
    "\n",
    "Since we now have our Twitter Developers login credentials (i.e. API keys and Access token), we can proceed to authenticating our program. First, we need to import the Twitter library, then create an Twitter.API object with the credentials from the ‚Äúsafe‚Äù place we talked about, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1474cac50577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3h2dbtJm7BchEn8NhszxhqGXF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kLLwKYTIZg4FyNlECC3jxI34ITTTMI22KWuq3uZjpzZid2kLVR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_token_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3395163311-L3kfrmBLjeT2wid9IUIrmhWhg9xhwoWIuGVdtdB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_token_secret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c7yzOy6ZP7fSspILaPkPFhaIp65P9g2rzcSTqG4OgSqDD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVerifyCredentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "twitter_api = twitter.api(consumer_key='3h2dbtJm7BchEn8NhszxhqGXF', consumer_secret='kLLwKYTIZg4FyNlECC3jxI34ITTTMI22KWuq3uZjpzZid2kLVR', access_token_key='3395163311-L3kfrmBLjeT2wid9IUIrmhWhg9xhwoWIuGVdtdB', access_token_secret='c7yzOy6ZP7fSspILaPkPFhaIp65P9g2rzcSTqG4OgSqDD')\n",
    "print(twitter_api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION TO RAFA: WHY DOESN'T LINE ABOVE WORK?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line in the previous code snippet is only there to verify that our API instance works. This will be determined based on the output we get. Run the above code and you should get something like the following JSON response:\n",
    "\n",
    "{\"created_at\": \"Tue Feb 12 17:48:27 +0800 2019\" 'default_profile\": true ............}\n",
    "\n",
    "That is nothing crazy but some data about the access made to the API through your Twitter account. If you reach this, you‚Äôre good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A.3: Creating the function to build the Test set**\n",
    "\n",
    "Now we can start on making a function that downloads the Test set that we talked about. Basically, this is going to be a function that takes a search keyword (i.e. string) as an input, searches for tweets that include this keyword and returns them as twitter.Status objects that we can iterate through.\n",
    "\n",
    "The caveat here, though, is that Twitter limits the number of requests you can make through the API for security purposes. This limit is 180 requests per 15-minute window.\n",
    "This means, we can only get up to 180 tweets using our search function every 15 minutes, which should not be a problem, as our Training set is not going to be that large anyway. For the sake of simplicity, we will limit the search to 100 tweets for now, not exceeding the allowed number of requests. Our function for searching for the tweets (i.e. Test set) will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTestSet(search_keyword):\n",
    "    try: \n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100)\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"text\":status.text, \"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have expected, this function will return a list of tweets that contain our search keyword.\n",
    "\n",
    "Note that we coupled ‚Äî into a JSON object ‚Äî every tweet‚Äôs text with a label that is NULL for now. This is merely because we are going to classify each tweet as Positive or Negative later on, in order to determine whether the sentiment on the search term is positive or negative, based on the majority count. This is how Sentiment Analysis pragmatically works.\n",
    "\n",
    "Before we move on, let‚Äôs test out our function by adding the following code after the function body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search keyword:trump\n",
      "Unfortunately, something went wrong..\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46dfec483c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtestDataSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildTestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "search_term = input(\"Enter a search keyword:\")\n",
    "testDataSet = buildTestSet(search_term)\n",
    "\n",
    "print(testDataSet[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation: THIS LINE DOESN'T WORK PROBABLY BECAUSE LINE 4 FAILED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print out five tweets that contain our search keyword on the Terminal of your IDE (if you‚Äôre using one). Now everything is set. We have our Test set and we can move on to building our Training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Preparing The Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that so far, we haven‚Äôt written a lot of code. This is attributed to the beauty of Python‚Äôs succinctness in syntax as well as the use of external program-ready libraries, such as RESTful APIs (Twitter API in our case).\n",
    "\n",
    "In this section, we will also be using our Twitter API instance from the last section. However, we need to get some things out the way first. We will be using a downloadable Training set. The tweets of which were all labeled as positive or negative, depending on the content. This exactly what a Training set is for.\n",
    "\n",
    "\"A Training set is critical to the success of the model. Data is which needs to be labeled properly with no inconsistencies or incompleteness, as training will rely heavily on the accuracy of such data and the manner of acquisition.\"\n",
    "\n",
    "For this task, we will be using the amazing Niek Sanders‚Äô Corpus of over 5000 hand-classified tweets, which makes it quite reliable. There‚Äôs also a catch here. Twitter does not allow storing tweets on a personal device, even though all such data is publicly available. Therefore, the corpus includes a keyword (topic of the tweet), a label (pos/neg) and a tweet ID number for every tweet (i.e. row in our CSV corpus). You can get the file containing the corpus from the original site, or through this link of a personal repository.\n",
    "\n",
    "Let‚Äôs backtrack for a bit. Remember the Twitter API limit we talked about? This will also apply here, as we will be using the API to get the actual tweet text through each tweet‚Äôs ID number included in the Corpus we have. This means, to download 5000 tweets, we will need to follow:\n",
    "\n",
    "max_number_of_requests = 180\n",
    "time_window = 15 minutes = 900 seconds\n",
    "Therefore, the process should follow:\n",
    "Repeat until end-of-file: {\n",
    "    180 requests -> (900/180) sec wait\n",
    "}\n",
    "\n",
    "Let‚Äôs now write the code that does exactly that. Let‚Äôs not forget to save the tweets we retrieve through the API into a new CSV file so that we don‚Äôt have to download them every time we run the code. Our function will be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrainingSet(corpusFile, tweetDataFile):\n",
    "    import csv\n",
    "    import time\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    with open(corpusFile,'rb') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',', quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            corpus.append({\"tweet_id\":row[2], \"label\":row[1], \"topic\":row[0]})\n",
    "    \n",
    "    rate_limit = 180\n",
    "    sleep_time = 900/180\n",
    "    \n",
    "    trainingDataSet = []\n",
    "    \n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = twitter_api.GetStatus(tweet[\"tweet_id\"])\n",
    "            print(\"Tweet fetched\" + status.text)\n",
    "            tweet[\"text\"] = status.text\n",
    "            trainingDataSet.append(tweet)\n",
    "            time.sleep(sleep_time) \n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "        # now we write them to the empty CSV file\n",
    "    with open(tweetDataFile,'wb') as csvfile:\n",
    "        linewriter = csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for tweet in trainingDataSet:\n",
    "            try:\n",
    "                linewriter.writerow([tweet[\"tweet_id\"], tweet[\"text\"], tweet[\"label\"], tweet[\"topic\"]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return trainingDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a handful, but it‚Äôs fairly simple if we break it down into parts. Firstly, we define the function to take two inputs, both of which are file paths:\n",
    "\n",
    "- corpusFile is the string path to the Niek Sanders‚Äô CSV corpus file we downloaded. This file, as mentioned earlier, includes the tweet‚Äôs topic, label and id.\n",
    "- tweetDataFile is the string path to the file we would like to save the full tweets in. In contrast to corpusFile, this file will include every tweet‚Äôs text as well as topic, label and id.\n",
    "\n",
    "Next, we started with an empty list corpus. We then opened the file corpusFile and appended every tweet from the file to the list corpus.\n",
    "\n",
    "The next segment of the code deals with getting the text of tweets based on the IDs. We loop through the tweets in corpus, calling the API on every tweet to get the Tweet.Status object of the particular tweet. Afterwards, we use that same object (status) to get the text associated with it and push it into the trainingDataSet then sleep (i.e. pause execution) for five minutes (900/180 seconds) in order to abide by the request limit we talked about.\n",
    "\n",
    "Now let‚Äôs take the time to leave our script download the tweets (which will take hours) following our last function. We can do this using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/colinechabloz/Desktop/CapstoneProject2/corpus.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-11b138409fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtweetDataFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/colinechabloz/Desktop/CapstoneProject2/tweetDataFile.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainingData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildTrainingSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetDataFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-83ff68ff6da1>\u001b[0m in \u001b[0;36mbuildTrainingSet\u001b[0;34m(corpusFile, tweetDataFile)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlineReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlineReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/colinechabloz/Desktop/CapstoneProject2/corpus.csv'"
     ]
    }
   ],
   "source": [
    "corpusFile = \"/Users/colinechabloz/Desktop/CapstoneProject2/corpus.csv\"\n",
    "tweetDataFile = \"/Users/colinechabloz/Desktop/CapstoneProject2/tweetDataFile.csv\"\n",
    "\n",
    "trainingData = buildTrainingSet(corpusFile, tweetDataFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as the code finishes executing, you will have your tweetDataFile CSV file full of tweets (~5000, as a matter of fact). If you got this far, CONGRATULATIONS !‚Äî it took me, the first time, a substantial amount of time to reach here without problems. Now we‚Äôre done with the relatively boring part. Let‚Äôs get ourselves hyped up for the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Pre-processing Tweets in The Data Sets\n",
    "\n",
    "Before we move on to the actual classification section, there is some cleaning up to do. As a matter of fact, this step is critical and usually takes a long time when building Machine Learning models. However, this will not be a problem in our task, as the data we have is relatively consistent. In other words, we know exactly what we need from it. I will express on this matter later on.\n",
    "\n",
    "Let‚Äôs talk about what matters and what doesn‚Äôt matter in Sentiment Analysis. Words are the most important part (to an extent that we will talk about in the upcoming section). However, when it comes to things like punctuation, you cannot get the sentiment from punctuation. Therefore, punctuation does not matter to Sentiment Analysis. Moreover, tweet components like images, videos, URLs, usernames, emojis, etc. do not contribute to the polarity (whether it is positive or negative) of the tweet. However, this is only true for this application. For instance, in another application, you could have a Deep Learning image classifier that learns and predicts whether this image that the tweet contains stands for something positive (e.g. a rainbow) or negative (e.g. a tank). When it comes to the technicality, both Sentiment Analysis and Deep Learning fall under Machine Learning. In fact, you can perform Sentiment Analysis through Deep Learning, but that‚Äôs a story for another day.\n",
    "\n",
    "So we know what we need to keep in the tweets we have and what we need to take out. This applies to both Training and Test sets. So let‚Äôs make a our pre-processor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a handful, so let‚Äôs break it down into parts. We start off by our imported libraries. re is Python‚Äôs Regular Expressions (RegEx) library, which takes care of parsing strings and modifying them in an efficient way without having to explicitly iterate through the characters comprising the particular string. We also imported ntlk, is the Natural Processing Toolkit, which is one of the most commonly used Python libraries out there. It takes care of any processing that we need to perform on text to change its form or extract certain components from it. The class constructor removes stop words. This is a relatively big topic that you can read up on later, as it is more into Natural Language Processing and less related to our topic.\n",
    "\n",
    "The processTweets function just loops through all the tweets input into it, calling its neighboring function processTweet on every tweet in the list. The latter does the actual pre-processing by first making all the text in lower-case letters. This is merely because, in almost all programming languages, ‚ÄúcAr‚Äù is not interpreted the same way as ‚Äúcar‚Äù. Therefore, it is better to normalize all characters to be lower-case across all our data. Secondly, URLs and usernames are removed from the tweet. This is for the reasons we disclosed earlier in the article. Afterwards, the number sign (i.e. #) is removed from every hashtag, in order to avoid hashtags being processed differently. Last but not least, duplicate characters are rid off of, in order to ensure that no important word goes unprocessed even if it is spelled out in an unusual way (e.g. ‚Äúcaaaaar‚Äù becomes ‚Äúcar‚Äù). Finally, the tweet‚Äôs text is broken into words (tokenized) in order to ease its processing in the upcoming stages.\n",
    "\n",
    "Let‚Äôs take an example. The following tweet could be present in the data set:\n",
    "\n",
    "\"@person1 retweeted @person2: Corn has got to be the most delllllicious crop in the world!!!! #corn #thoughts...\"\n",
    "\n",
    "Our pre-processor will result in the tweet looking like:\n",
    "\n",
    "‚ÄúAT_USER rt AT_USER corn has got to be the most delicious crop in the world corn thoughts‚Äù\n",
    "\n",
    "And finally, the tokenization will result in:\n",
    "\n",
    "{‚Äúcorn‚Äù, ‚Äúmost‚Äù, ‚Äúdelicious‚Äù, ‚Äúcrop‚Äù, ‚Äúworld‚Äù, ‚Äúcorn‚Äù, ‚Äúthoughts‚Äù}\n",
    "\n",
    "Note that our code removed duplicate characters in words as we metioned earlier (i.e. ‚Äúdelllllicious‚Äù became ‚Äúdelicious‚Äù). However, it did not remove duplicate words (i.e. ‚Äúcorn‚Äù) from the text, but rather kept them. This is because duplicate word play a role in determining the polarity of the text (as we will see in the upcoming section).\n",
    "\n",
    "We are all set to use our Pre-processor class. First, we will create a variable that refers to it (an object), and then call it on both the Training and Test sets as we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/colinechabloz/nltk_data'\n    - '/Users/colinechabloz/anaconda3/nltk_data'\n    - '/Users/colinechabloz/anaconda3/share/nltk_data'\n    - '/Users/colinechabloz/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/colinechabloz/nltk_data'\n    - '/Users/colinechabloz/anaconda3/nltk_data'\n    - '/Users/colinechabloz/anaconda3/share/nltk_data'\n    - '/Users/colinechabloz/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-acc96635cf33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetProcessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreprocessedTrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweetProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreprocessedTestSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweetProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-43af3c80c68a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreProcessTweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'AT_USER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprocessedTweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/colinechabloz/nltk_data'\n    - '/Users/colinechabloz/anaconda3/nltk_data'\n    - '/Users/colinechabloz/anaconda3/share/nltk_data'\n    - '/Users/colinechabloz/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tweetProcessor = PreProcessTweets()\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(trainingData)\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to the most exciting part ‚Äî classification. But first, let‚Äôs brush up (or touch) on our algorithm for this task: Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: Naive Bayes Classifier\n",
    "\n",
    "Without a doubt, one of the most important concepts in Computer Science and Machine Learning. It can be frustrating to get into the math of it head-first. Therefore, I will not be explaining it from the mathematical perspective.\n",
    "\n",
    "\"You don‚Äôt need to know the math to be a Computer Scientist. However, you do need to know the math to become a really good one.\"\n",
    "\n",
    "I want to emphasize on the fact that I will only briefly explain Naive Bayes Classification here, as the in-depth explanation deserves its own lengthy post.\n",
    "\n",
    "Naive Bayes Classifier is a classification algorithm that relies on Bayes‚Äô Theorem. This theorem provides a way of calculating a type or probability called posterior probability, in which the probability of an event A occurring is reliant on probabilistic known background (e.g. event B evidence). For example, if Person_X only plays tennis when it is not raining outside, then, according to Bayesian statistics, the probability of Person_X playing tennis when it is not raining can be given as:\n",
    "\n",
    "P(X plays | no rain) = P(no rain | X plays)*P(x plays)/P(no rain)\n",
    "\n",
    "following Bayes‚Äô theorem:\n",
    "\n",
    "P(A|B) = P(B|A)*P(A)/P(B)\n",
    "\n",
    "All you need to know for our task is that a Naive Bayes Classifier depends on the ever-famous Bayes‚Äô theorem. Before we move on, let‚Äôs give a quick overview of the steps we will be taking next:\n",
    "\n",
    "- Build a vocabulary (list of words) of all the words resident in our training data set.\n",
    "- Match tweet content against our vocabulary ‚Äî word-by-word.\n",
    "- Build our word feature vector.\n",
    "- Plug our feature vector into the Naive Bayes Classifier.\n",
    "\n",
    "This might seem like a lot, but don‚Äôt worry. It is actually fairly simple and as short as it can be. Let‚Äôs take it down step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step D.1: Building the vocabulary**\n",
    "\n",
    "A vocabulary in Natural Language Processing is a list of all speech segments available for the model. In our case, this includes all the words resident in the Training set we have, as the model can make use of all of them relatively equally ‚Äî at this point, to say the least. The code will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(preprocessedTrainingData):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in preprocessedTrainingData:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just creating a list of all_words we have in the Training set, breaking it into word features. Those word_features are basically a list of distinct words, each of which has its frequency (number of occurrences in the set) as a key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step D.2: Matching tweets against our vocabulary**\n",
    "\n",
    "This step is crucial, as we will go through all the words in our Training set (i.e. our word_features list), comparing every word against the tweet at hand, associating a number with the word following:\n",
    "\n",
    "\"label 1 (true): if word in vocabulary is resident in tweet\n",
    "label 0 (false): if word in vocabulary is not resident in tweet\"\n",
    "\n",
    "This is fairly simple to code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the last snippet, for every word in the word_features, we will have the JSON key ‚Äòcontains word X‚Äô, where X is the word. Every key of those will have the value True/False, according to what we said earlier about the labels ‚Äî True for ‚Äòpresent‚Äô and False for ‚Äòabsent‚Äô."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step D.3: Building our feature vector**\n",
    "\n",
    "Let‚Äôs now call the last two functions we have written. This will build our final feature vector, with which we can proceed on to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessedTrainingData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-43260fd800f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessedTrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainingFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessedTrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessedTrainingData' is not defined"
     ]
    }
   ],
   "source": [
    "word_features = buildVocabulary(preprocessedTrainingData)\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NTLK built-in function apply_features does the actual feature extraction from our lists. Our final feature vector is trainingFeatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step D.4: Training the classifier**\n",
    "\n",
    "We have finally come to the most important ‚Äî and ironically the shortest ‚Äî part of our task. Thanks to NLTK, it will only take us a function call to train the model as a Naive Bayes Classifier, since the latter is built into the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-9475ed3c0552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNBayesClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainingFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôre almost done! All we have left is running the classifier training code (i.e. nltk.NaiveBayesClassifier.train()) and testing it. Note that this code could take a few minutes to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section E: Testing The Model\n",
    "\n",
    "Moment of truth! Let‚Äôs finish up our work by running the classifier (i.e. NBayesClassifier) on the 100 tweets that we downloaded from Twitter, according to our search term, and getting the majority vote of the labels returned by the classifier, then outputting the total positive or negative percentage (i.e. score) of the tweets. This is also going to be very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessedTestDataSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-3ec43fafecb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNBResultLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessedTestDataSet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get the majority vote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNBResultLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNBResultLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall Positive Sentiment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessedTestDataSet' is not defined"
     ]
    }
   ],
   "source": [
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestDataSet]\n",
    "\n",
    "# get the majority vote\n",
    "if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "    print(\"Overall Positive Sentiment\")\n",
    "    print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "else: \n",
    "    print(\"Overall Negative Sentiment\")\n",
    "    print(\"Negative Sentiment Percentage = \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs it! Congratulations. You have created a Twitter Sentiment Analysis Python program. Note that we did not touch on the accuracy (i.e. evaluate the model) because it is not our topic for the day. There will be a post where I explain the whole model/hypothesis evaluation process in Machine Learning later on. Let‚Äôs take a final look at the full code we wrote for this task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Sentiment Analysis is an interesting way to think about the applicability of Natural Language Processing in making automated conclusions about text. It is being utilized in social media trend analysis and, sometimes, for marketing purposes. Making a Sentiment Analysis program in Python is not a difficult task, thanks to modern-day, ready-for-use libraries. This program is a simple explanation to how this kind of application works. This is only for academic purposes, as the program described here is by no means production-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3  DataCamp The Twitter API and Authentification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Authentication**\n",
    "\n",
    "The package tweepy is great at handling all the Twitter API OAuth Authentication details for you. All you need to do is pass it your authentication credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"3395163311-L3kfrmBLjeT2wid9IUIrmhWhg9xhwoWIuGVdtdB\"\n",
    "access_token_secret = \"c7yzOy6ZP7fSspILaPkPFhaIp65P9g2rzcSTqG4OgSqDD\"\n",
    "consumer_key = \"3h2dbtJm7BchEn8NhszxhqGXF\"\n",
    "consumer_secret = \"kLLwKYTIZg4FyNlECC3jxI34ITTTMI22KWuq3uZjpzZid2kLVR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tweet stream listener class\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.file = open(\"tweets.txt\", \"w\")\n",
    "\n",
    "    def on_status(self, status):\n",
    "        tweet = status._json\n",
    "        self.file.write( json.dumps(tweet) + '\\n' )\n",
    "        self.num_tweets += 1\n",
    "        if self.num_tweets < 100:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        self.file.close()\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming tweets**\n",
    "\n",
    "Now that you have set up your authentication credentials, it is time to stream some tweets! We have already defined the tweet stream listener class, MyStreamListener, just as Hugo did in the introductory video. You can find the code for the tweet stream listener class here.\n",
    "\n",
    "Your task is to create the Streamobject and to filter tweets according to particular keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track=['clinton','trump','sanders','cruz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and explore your Twitter data**\n",
    "\n",
    "Now that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next few interactive exercises. In this exercise, you'll read the Twitter data into a list: tweets_data.\n",
    "\n",
    "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_path = 'tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['created_at', 'id', 'id_str', 'text', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'retweeted_status', 'is_quote_status', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms'])\n"
     ]
    }
   ],
   "source": [
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter data to DataFrame**\n",
    "\n",
    "Now you have the Twitter data in a list of dictionaries, tweets_data, where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet. The text in a tweet, t1, is stored as the value t1['text']; similarly, the language is stored in t1['lang']. Your task is to build a DataFrame in which each row is a tweet and the columns are 'text' and 'lang'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text lang\n",
      "0  RT @anuraag_saxena: #Democrat comes in dressed...   en\n",
      "1  RT @Strandjunker: Love Hillary, hate Hillary, ...   en\n",
      "2       In Trump‚Äôs America, the academics, supported   en\n",
      "3  RT @BobMagaw: Donald Trump‚Äôs unique speaking s...   en\n",
      "4  RT @JonAshworth: üö®So from his Downing Street o...   en\n"
     ]
    }
   ],
   "source": [
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text','lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A little bit of Twitter text analysis**\n",
    "\n",
    "Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words 'clinton', 'trump', 'sanders' and 'cruz'. In the pre-exercise code, we have defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting your Twitter data**\n",
    "\n",
    "Now that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the statistical data visualization library seaborn, which you may not have seen before, but we'll guide you through. You'll first import seaborn as sns. You'll then construct a barplot of the data using sns.barplot, passing it two arguments:\n",
    "\n",
    "a list of labels and\n",
    "a list containing the variables you wish to plot (clinton, trump and so on.)\n",
    "Hopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot histogram\n",
    "ax = sns.barplot(cd, [clinton, trump, sanders, cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tweets.txt is located in the same folder as Jupyter Notebook is located.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
